# YARNN
Yet another RNN/LSTM. The notations/equations follow the [Deep Learning](http://www.deeplearningbook.org) book.

Example output from a character RNN language model trained on a tiny corpus:
```
epoch 89, loss 1579.447497
----
 τ for each member of crins in time-delay neural networks than of the output.
 Each member of the previous members of the outpuen neural networks valke of
 teadisity to Nepal in time. Such sharing mami-l
----
```

word level model (notice the overfitting):
```
epoch 199, loss 1.332666
----
 recurrent neural networks or readily scale to Nepal in 2009” data “In even is a
 sequence where each member of the output is a that network with a sequence where
 each member of the output is a function of a small number of neighboring members
 of the input . it across convolution positions on large width and height , and
 some not networks ( Lang and Hinton , 1988 ; Waibel et al. , 1986a ) are produced
 sequences , with ; . value al. , 1986a ) are a family of neural networks for processing
 sequential data .
----
```


References:

http://www.deeplearningbook.org/contents/rnn.html

http://karpathy.github.io/2015/05/21/rnn-effectiveness/

http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/
